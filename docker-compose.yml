services:
  ner-gpu:
    profiles: ["gpu"]
    build:
      context: .
      dockerfile: docker/Dockerfile.gpu
    image: repo-ner-gpu
    environment:
      # пути к модели — подставил те, что реально есть на сервере
      - MODEL_DIR=/models/ner_xlmr_wordlevel_entity
      - LABELS_PATH=/models/ner_xlmr_wordlevel_entity/labels.txt

      # параметры инференса/батчинга
      - MAX_SEQ_LEN=64
      - BATCH_MAX_SIZE=32
      - BATCH_TIMEOUT_MS=8
      - USE_QUEUE=true
      - TOKENIZERS_PARALLELISM=false

      # опционально: включены твои правила препроцессинга
      - PP_TOKEN_INJECT_REGEX=true
      - PP_TOKEN_INJECT_VOLUME_LEVENSHTEIN=true
      - PP_TOKEN_NULLIFY_AFTER_PREPOSITIONS=true
      - PP_TOKEN_NULLIFY_IF_STARTS_WITH_ALL=true
      - PP_TOKEN_ENSURE_LEADING_WORD_O=true
      - PP_WORD_RULES_ENABLED=true
      - PP_WORD_NULLIFY_COUNT_AFTER_PREP=2

      # важно для nvidia runtime
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

    volumes:
      - ../models:/models:rw

    # для Compose v2: просим GPU через deploy.reservations + запуск с --compatibility
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    ports:
      - "8000:8000"